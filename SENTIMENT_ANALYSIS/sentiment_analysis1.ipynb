{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611c3de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.73      0.56      0.63      2011\n",
      "         0.0       0.85      0.55      0.67      2941\n",
      "         1.0       0.63      0.89      0.74      3668\n",
      "\n",
      "    accuracy                           0.70      8620\n",
      "   macro avg       0.74      0.67      0.68      8620\n",
      "weighted avg       0.73      0.70      0.69      8620\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the Twitter sentiment dataset\n",
    "data = pd.read_csv('twitter_data.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Preprocess the data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords and perform lowercasing\n",
    "data['Cleaned_Text'] = data['clean_text'].apply(lambda x: ' '.join([word.lower() for word in str(x).split() if word.lower() not in stop_words]))\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Cleaned_Text'], data['category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to numerical features using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_NB = naive_bayes.predict(X_test_vectorized)\n",
    "\n",
    "# Generate classification report\n",
    "report_NB = classification_report(y_test, y_pred_NB)\n",
    "print(report_NB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7185d800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>modi promised “minimum government maximum gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>talk nonsense continue drama vote modi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>say vote modi welcome bjp told rahul main camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>asking supporters prefix chowkidar names modi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>answer among powerful world leader today trump...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category  \\\n",
       "0  when modi promised “minimum government maximum...      -1.0   \n",
       "1  talk all the nonsense and continue all the dra...       0.0   \n",
       "2  what did just say vote for modi  welcome bjp t...       1.0   \n",
       "3  asking his supporters prefix chowkidar their n...       1.0   \n",
       "4  answer who among these the most powerful world...       1.0   \n",
       "\n",
       "                                        Cleaned_Text  \n",
       "0  modi promised “minimum government maximum gove...  \n",
       "1             talk nonsense continue drama vote modi  \n",
       "2  say vote modi welcome bjp told rahul main camp...  \n",
       "3  asking supporters prefix chowkidar names modi ...  \n",
       "4  answer among powerful world leader today trump...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03c652c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.tree import DecisionTreeClassifier\\nclf=DecisionTreeClassifier()\\nclf.fit(X_train_vectorized,y_train)\\ny_pred_DT=clf.predict(X_test_vectorized)\\nreport_DT = classification_report(y_test, y_pred_DT)\\nprint(report_DT)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier()\n",
    "clf.fit(X_train_vectorized,y_train)\n",
    "y_pred_DT=clf.predict(X_test_vectorized)\n",
    "report_DT = classification_report(y_test, y_pred_DT)\n",
    "print(report_DT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcd94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9959ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv('twitter_data.csv')\n",
    "dataset['category'] = dataset['category'].map({-1: 0, 0: 1, 1: 2})\n",
    "# Preprocess the dataset\n",
    "sentences = dataset['clean_text'].astype(str).values  # Convert to string\n",
    "labels = dataset['category'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "train_words = [sentence.lower().split() for sentence in train_sentences]\n",
    "test_words = [sentence.lower().split() for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d792025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(train_words, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(w2v_model.wv.index_to_key)\n",
    "\n",
    "# Convert words to indices\n",
    "train_sequences = [[w2v_model.wv.key_to_index[word] for word in sentence if word in w2v_model.wv.key_to_index] for sentence in train_words]\n",
    "test_sequences = [[w2v_model.wv.key_to_index[word] for word in sentence if word in w2v_model.wv.key_to_index] for sentence in test_words]\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_length = max(max(len(train_seq), len(test_seq)) for train_seq, test_seq in zip(train_sequences, test_sequences))\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c68efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  2. nan]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(train_labels)\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72562fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = np.where(train_labels < 3)  # Filter out invalid labels\n",
    "train_labels = train_labels[valid_indices]\n",
    "train_sequences = train_sequences[valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03943534",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -2147483648 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15752\\3408114728.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Convert labels to one-hot vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_labels_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtest_labels_one_hot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -2147483648 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length, trainable=True))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming 3 classes: negative, neutral, positive\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot vectors\n",
    "train_labels_one_hot = to_categorical(train_labels, num_classes=3)\n",
    "test_labels_one_hot = to_categorical(test_labels, num_classes=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_sequences, train_labels_one_hot, validation_data=(test_sequences, test_labels_one_hot), epochs=10, batch_size=16384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c470be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_LSTM=model.predict(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249372d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bd1330",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_LSTM=[]\n",
    "for i in range(len(y_hat_LSTM)):\n",
    "    y_pred_LSTM.append(np.argmax(y_hat_LSTM[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(test_labels, y_pred_LSTM)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Generate classification report\n",
    "class_labels = ['negative', 'neutral', 'positive']\n",
    "report = classification_report(test_labels, y_pred_LSTM, target_names=class_labels)\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "confusion_mat = confusion_matrix(test_labels, y_pred_LSTM)\n",
    "sns.heatmap(confusion_mat,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import logging\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7629517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('twitter_data.csv')\n",
    "df['category'] = df['category'].map({-1: 0, 0: 1, 1: 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_var = \"\"\n",
    "def token_text(text):\n",
    "  marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "  # Tokenize our sentence with the BERT tokenizer.\n",
    "  tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "  # Print out the tokens.\n",
    " # print (tokenized_text)\n",
    "\n",
    "  # Display the words with their indeces.\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  for tup in zip(tokenized_text, indexed_tokens):\n",
    "      print('{:<12} {:>6,}'.format(tup[0], tup[1])) \n",
    "  segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "  print (segments_ids)\n",
    "\n",
    "  # Convert inputs to PyTorch tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "  # Load pre-trained model (weights)\n",
    "  model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensors)\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bed4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text(df['clean_text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56284c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "dataset = ...  # Load your financial_news dataset here\n",
    "sentences = df['clean_text'].tolist()\n",
    "sentiment_labels = df['category'].tolist()\n",
    "\n",
    "# Define the parameters\n",
    "max_seq_length = 100  # Maximum sequence length\n",
    "num_classes = 3  # Number of sentiment classes (positive, negative, neutral)\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sentences,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = tf.concat(input_ids, axis=0)\n",
    "attention_masks = tf.concat(attention_masks, axis=0)\n",
    "sentiment_labels = tf.one_hot(sentiment_labels, depth=num_classes)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_ratio = 0.8  # 80% for training, 20% for testing\n",
    "split_index = int(len(input_ids) * split_ratio)\n",
    "\n",
    "train_input_ids = input_ids[:split_index]\n",
    "train_attention_masks = attention_masks[:split_index]\n",
    "train_labels = sentiment_labels[:split_index]\n",
    "test_input_ids = input_ids[split_index:]\n",
    "test_attention_masks = attention_masks[split_index:]\n",
    "test_labels = sentiment_labels[split_index:]\n",
    "\n",
    "# Load the BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Freeze the BERT layers\n",
    "bert_model.trainable = False\n",
    "\n",
    "# Define the model architecture\n",
    "input_ids_layer = Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "attention_masks_layer = Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_masks_layer)[0]\n",
    "pooling_layer = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "output_layer = Dense(units=num_classes, activation='softmax')(pooling_layer)\n",
    "\n",
    "model = Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "model.summary()\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "    [train_input_ids, train_attention_masks],\n",
    "    train_labels,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_data=([test_input_ids, test_attention_masks], test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13794b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
